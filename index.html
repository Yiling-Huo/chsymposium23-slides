<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>The (perhaps) limited role of phonology in prediction during language comprehension</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white-cus.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
					<div class="qrcode"></div>
					<h3>The (perhaps) limited role of phonology in prediction during language comprehension</h3>
					<p>Yiling Huo</p>
					<p>1st Chandler House Symposium</p>
				</section>
				<section data-auto-animate>
					<h4>Generation of predictions might be a universal principle of the brain:</h4>
					<aside class="notes">
						Evidence from all kinds of neuroscience suggests that prediction might be a universal principle of the brain. For example, if a ball flies towards us, we are able to dodge because based on our past experiences, we can predict its trajectory, and remove ourselves from it. This is an example of prediction in action planning. 
					</aside>
				</section>
				<section data-auto-animate>
					<h4>Generation of predictions might be a universal principle of the brain:</h4>
					<img src="./images/ball.svg"  width="20%">
					<aside class="notes">
						Evidence from all kinds of neuroscience suggests that prediction might be a universal principle of the brain. For example, if a ball flies towards us, we are able to dodge because based on our past experiences, we can predict its trajectory, and remove ourselves from it. This is an example of prediction in action planning. 
					</aside>
				</section>
				<section data-markdown data-auto-animate data-auto-animate-id="2" data-separator-notes="^Note:">
					#### Prediction during language processing:

					Note:

					Now, prediction has also been found in language comprehension. For example, if I say to you...

					---

					#### Prediction during language processing:

					"This morning I went to Starbucks to buy ..."

					Note:

					Now, prediction has also been found in language comprehension. For example, if I say to you...
				</section>
				
				<section data-auto-animate data-auto-animate-id="2">
					<h4>Prediction during language processing:</h4>

					<p>"This morning I went to Starbucks to buy ... <span style="color: #EC4899;">coffee</span>"</p>
					<aside class="notes">
						Most of you will be thinking of coffee, before I actually say the word. That is an example of prediction during language comprehension or language processing. 
					</aside>
				</section>
				<section data-markdown data-auto-animate data-auto-animate-id="3" data-separator-notes="^Note:">
					#### What aspects of language are involved in prediction during language comprehension?

					Note:

					Now people have been researching what aspects of language are involved in the process of prediction during language comprehension? Turns out quite a lot of them:

					---

					#### What aspects of language are involved in prediction during language comprehension?

					- Sentential and discourse context (Kutas & Hillyard, 1984; Federmeier & Kutas, 1999)
					- Event knowledge / Theta roles (Altmann & Kamide, 1999; Chow et al., 2016)
					- Morphosyntax (gender/case markers) (Kamide et al., 2003; Lew-Williams & Fernald, 2007, 2010)
					- ...

					Note:

					Now people have been researching what aspects of language are involved in the process of prediction during language comprehension? Turns out quite a lot of them:
				</section>
				<section data-auto-animate data-auto-animate-id="3">
					<p style="text-align: left;">"I eat <strong>rice</strong>"</p>
					<div><img src="./images/comp-model.svg" width="40%"></div>
					<aside class="notes">
						<p>Now here's a very simplified version of comprehension. Listeners go through several levels of representation during comprehension. For example, when we get the word 'rice' in 'I eat rice', we first extract the phonological form 'rice' from the speech sound. And from the phonological form, we retrieve the lexical item 'rice' and all of its semantic features, and then we assign it the role of object/patient, and then we get the message 'I eat rice'. </p>

						<p>Now we've seen evidence for prediction based on the semantics and the syntax, but what about the phonology?</p>
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="4">
					<h3>Phonology in prediction?</h3>

					<p class="fragment" style="text-align: left;">DeLong et al. (2005)</p>
					<div class="fragment"><img src="./images/delong2005.png"  width="60%"></div>
					<aside class="notes">
						<p>My project is concerned with the role of phonology in prediction. </p>
						<p>The most famous piece of work on phonology in prediction comes from DeLong et al 2005. In this study they showed people context constraining sentences such as '..', and here you would expect the ending to be 'a kite', and 'an airplane' would be unexpected. And indeed DeLong et al found that the unexpected airplane elicited a larger N400 than kite, which means it took more effort to process airplane than kite. And more importantly, they found that the preceding indefinite article an before airplane also elicited a larger N400 compared with the expected a. So not only were people predicting kite, when they get an indefinite article that's not compatible with kite, their brain notices it and takes more effort to process the article as well. </p>
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="4">
					<h3>Phonology in prediction?</h3>

					<p class="fragment" style="text-align: left;">However: Nieuwland et al. (2018) (N=356)</p>
					<div class="fragment"><img src="./images/nieuwland2018.jpg"  width="60%"></div>
					<aside class="notes">
						<p>Since DeLong et al published their results, people have been trying to replicate the same effect. The most important replication attempt was by Nieuwland et al 2018, where they did a multi-lab large scale replication of DeLong et al, with 356 participants. They managed to replicate the prediction effect on the nouns, but not on the articles. So it appears that with a large sample size, it's not necessarily true that people predict the phonology to the point that they can notice something wrong at an unexpected article. </p>
						<p>So the puzzle still remains, and the role of phonology in prediction is still under hot debate. </p>
					</aside>
				</section>
				<section>
					<h4>Two questions to ask:</h4>
					<ul>
						<li class="fragment">Can listeners use phonological information to generate predictions?</li>
						<li class="fragment">Can listeners detect prediction errors / revise their predictions using phonology?</li>
					</ul>
					<aside class="notes">
						Now if we want to study the role of anything in language prediction, there are at least these two questions that we can ask. '....' The studies that I've just talked about more or less fit under the second question. So they were trying to see if people 'notice' something wrong when the indefinite article was not compatible with their prediction of the nouns. Now on the first question, there actually aren't that many studies that directly address the first question, which brings me to my first two experiment.
					</aside>
				</section>
				<section>
					<h3>Experiments 1 and 2</h3>
					<aside class="notes">
						My experiments 1 and 2 address the question of whether people can use phonology to generate predictions. Here I'm using mandarin chinese tone sandhi as an example of phonological patterns that can be used to generate predictions.
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="5">
					<h4>Tone sandhi in Mandarin Chinese</h4>
					<p class="fragment" style="text-align: left;">Lexical tone: pitch contour that encodes meaning.</p>
					<p class="fragment" style="text-align: left;"><em>ma1</em> "mother", <em>ma2</em> "hemp", <em>ma3</em> "horse", <em>ma4</em> "scold"</p>
					<p class="fragment" style="text-align: left;"><em>ma1ma1-ma4-ma3</em> "Mom scolds the horse."</p>
					<aside class="notes">
						Let's start from tone sandhi patterns in mandarin chinese. Before we talk about tone sandhi, we need to talk about lexical tones, which I'm sure you are all familiar with. lexical tones are pitch contour that encodes meaning. Mandarin Chinese has four tones, and depending on the tone, the same segements can have different meanings. 
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="5">
					<h4>Tone sandhi in Mandarin Chinese</h4>
					<p class="fragment" data-fragment-index="1" style="text-align: left;">Tone sandhi: lexical tone can undergo change triggered by neighboring linguistic environments.</p>
					<p class="fragment" data-fragment-index="5" style="text-align: left;"><strong>The <em>yi</em> sandhi:</strong> numeral <em>yi1</em> "one"</p>
					<p class="fragment" data-fragment-index="6" style="text-align: left;"><em>yi4tian1</em>, <em>yi4tiao2</em>, <em>yi4ba3</em>, <span class="fragment" data-fragment-index="7" style="color: #EC4899;"><em><strong>yi2ge4</strong></em></span></p>
					<p class="fragment" data-fragment-index="4" style="text-align: left;"><strong>The T3 sandhi:</strong> <span class="fragment" data-fragment-index="8">numeral <em>liang3</em> "two"</span></p>
					<p class="fragment" data-fragment-index="2" style="text-align: left;"><em>xiao3gou3</em> &#8594 <span class="fragment" data-fragment-index="3" style="color: #EC4899;"><em><strong>xiao2gou3</strong></em></span></p>
					<aside class="notes">
						Tone sandhi refers to the phenomenon that lexical tones can undergo change triggered by neighboring lisguistic environments. These changes does not affect meaning. For example, in Mandarin, if you have two tone-3 syllables such as xiao3gou3, the first syllable needs to be changed into something very similar to tone-2. This is called the T3 sandhi. We have another sandhi in Mandarin which I call the yi sandhi. This sandhi says that the numeral yi, which means one, should be realised in tone-4 whenever another syllable follows it in the same phrase, except when it's followed by another tone-4, in this case it's realised in tone-2. My experiments will use these two tone sandhi at the numeral position, so the yi sandhi obviously happens with the numeral yi, and the T3 sandhi happens with the numeral liang3, which means two. 
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="6">
					<h4>Experiments 1 and 2</h4>
					<img src="./images/exp1yi.png" width="100%">
					<aside class="notes">
						Here is the design of experiments 1 and 2. Experiment 2 is a direct replication of experiment 1. And it'll become clear in a moment why I need to talk about these experiments separately. ...VWE...neutral context...conditions...prediction of results
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="6">
					<h4>Experiments 1 and 2</h4>
					<img src="./images/exp1liang.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h4 style="text-align: left;">Experiment 1 Results</h4>
					<p style="text-align: left;">The <em>yi</em> sandhi ("<em>yi</em>"):</p>
					<img src="./images/exp1res1.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h4 style="text-align: left;">Experiment 1 Results</h4>
					<p style="text-align: left;">The T3 sandhi ("<em>liang</em>"):</p>
					<img src="./images/exp1res2.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="7">
					<h4 style="text-align: left;">Experiment 1 Results</h4>
					<img src="./images/exp1res3.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="8">
					<h4 style="text-align: left;">Experiment 2 Results</h4>
					<p style="text-align: left;">The <em>yi</em> sandhi ("<em>yi</em>"):</p>
					<img src="./images/exp2res1.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="8">
					<h4 style="text-align: left;">Experiment 2 Results</h4>
					<p style="text-align: left;">The T3 sandhi ("<em>liang</em>"):</p>
					<img src="./images/exp2res2.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="8">
					<h4 style="text-align: left;">Experiment 2 Results</h4>
					<img src="./images/exp2res3.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="9">
					<h4>Experiments 1 and 2</h4>
					<p class="fragment" style="text-align: left;">Can listeners use tone sandhi to generate predictions?</p>
					<p class="fragment" style="text-align: left; color:rgb(70, 70, 255)">Perhaps, but this effect is not always replicated. </p>
					<aside class="notes">
						From experiments 1 and 2, we can try to answer the first question using tone sandhi: can people make predictions based on tone sandhi? Maybe, because we got the results from experiment 1, but this effect is not always replicated.
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="9">
					<p style="text-align: left;">Can listeners use tone sandhi to generate predictions?</p>
					<p style="text-align: left; color:rgb(70, 70, 255)">Perhaps, but this effect is not always replicated. </p>
					<p class="fragment" style="text-align: left;">Can listener use tone sandhi to detect prediction errors / revise their predictions?</p>
					<aside class="notes">
						Now we haven't answered the second question yet with tone sandhi. so can people use tone sandhi to detect prediction errors and or revise their predictions? Which brings me to Experiment 3. 
					</aside>
				</section>
				<section>
					<h3>Experiment 3</h3>
				</section>
				<section>
					<h4>Experiment 3</h4>
					<img src="./images/exp3.png" width="100%">
					<aside class="notes">
						Experiment 3 was also a visual world eye-tracking study. but this time listeners were listening to context constraining sentences such as... unexpected ending... conditions... prediction of results...
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="10">
					<h4 style="text-align: left;">Experiment 3 Results</h4>
					<p style="text-align: left;">The <em>yi</em> sandhi ("<em>yi</em>"):</p>
					<img src="./images/exp3res1.png" width="100%">
				</section>
				<section data-auto-animate data-auto-animate-id="10">
					<h4 style="text-align: left;">Experiment 3 Results</h4>
					<p style="text-align: left;">The T3 sandhi ("<em>liang</em>"):</p>
					<img src="./images/exp3res2.png" width="100%">
				</section>
				<section>
					<p style="text-align: left;">Can listeners use tone sandhi to generate predictions?</p>
					<p style="text-align: left; color:rgb(70, 70, 255)">Perhaps, but this effect is not always replicated. </p>
					<p style="text-align: left;">Can listener use tone sandhi to detect prediction errors / revise their predictions?</p>
					<p class="fragment" style="text-align: left; color:rgb(70, 70, 255)">No evidence for revising prediction using tone sandhi. </p>
				</section>
				<section>
					<h3>Take home message</h3>
					<p class="fragment">Phonology's role in prediction may be <span style="color:rgb(70, 70, 255)">limited</span>. </p>
					<aside class="notes">So the take home message is that phonology's role in prediction may be limited. We are sort of getting some weak evidence that it might be involved, but it looks like we are getting more evidence that it is not.</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="11">
					<h4>But why?</h4>
					<p>We see that many aspects of language are involved in prediction. Why not phonology?</p>
				</section>
				<section data-auto-animate data-auto-animate-id="11" data-visibility="hidden">
					<h4>But why?</h4>
					<p>We see that many aspects of language are involved in prediction. Why not phonology?</p>
					<p class="fragment" style="text-align: left;">Processing time hypothesis:</p>
						<ul style="font-size:80%">
							<li class="fragment">Evidence that English articles 'a/an' and Chinese tone sandhi are involved in prediction when <span style="color:rgb(70, 70, 255)">SOA is long</span> / <span style="color:rgb(70, 70, 255)">speech rate is low</span> (Ito et al., 2016; Liu et al., 2023). </li>
							<li class="fragment">Maybe the comprehender needs <span style="color:rgb(70, 70, 255)">more processing time</span> in order to get phonology involved in prediction. </li>
						</ul>
					<aside class="notes">
						But why? we've seen that people use all sorts of other aspects of language to predict such as semantics and syntax, but why not phonology? Now here's some hypotheses that I'm currently working on. So this is still ongoing work and I'm just going to briefly mention them here without going into details. The first hypothesis...
					</aside>
				</section>
				<section data-auto-animate data-auto-animate-id="11" data-visibility="hidden">
					<h4>But why?</h4>
					<p>We see that many aspects of language are involved in prediction. Why not phonology?</p>
					<p class="fragment" style="text-align: left;">The 'Filter' hypothesis:</p>
						<ul style="font-size:80%">
							<li class="fragment">As language involves <span style="color:rgb(70, 70, 255)">several levels of representation</span> that require different levels of abstraction (phonology - semantics - syntax),</li>
							<li class="fragment">Maybe only information at <span style="color:rgb(70, 70, 255)">a certain level of abstraction or above</span> (e.g. semantics and above) is involved in prediction (under normal circumstances). </li>
						</ul>
					<aside class="notes">
						I call the second hypothesis the 'filter' hypothesis. This hypothesis is not contradictory to the first hypothesis, bur rather it's trying to answer the question from a different angle. .... whatever is in control of the predictive behaviour in language starts from the highest level of representation, and maybe it can only project down to a certain level that's above phonology under normal circumstances.
					</aside>
				</section>
				<section>
					<h2>Thank you!</h2>
					<img src="./dist/theme/qr.png" width="12%">
				</section>
				<section style="text-align: left; font-size:30%;">
					<h2>References</h2>
					<p>Altmann, G. T., & Kamide, Y. (1999). Incremental interpretation at verbs: Restricting the domain of subsequent reference. <em>Cognition</em>, 73(3), 247-264.</p>
					<p>Chow, W. Y., Smith, C., Lau, E., & Phillips, C. (2016). A “bag-of-arguments” mechanism for initial verb predictions. <em>Language, Cognition and Neuroscience</em>, 31(5), 577-596.</p>
					<p>DeLong, K. A., Urbach, T. P., & Kutas, M. (2005). Probabilistic word pre-activation during language comprehension inferred from electrical brain activity. <em>Nature neuroscience</em>, 8(8), 1117-1121.</p>
					<p>Federmeier, K. D., & Kutas, M. (1999). A rose by any other name: Long-term memory structure and sentence processing. <em>Journal of memory and Language</em>, 41(4), 469-495.</p>
					<p>Ito, A., Corley, M., Pickering, M. J., Martin, A. E., & Nieuwland, M. S. (2016). Predicting form and meaning: Evidence from brain potentials. <em>Journal of Memory and Language</em>, 86, 157-171.</p>
					<p>Kamide, Y., Altmann, G. T., & Haywood, S. L. (2003). The time-course of prediction in incremental sentence processing: Evidence from anticipatory eye movements. <em>Journal of Memory and language</em>, 49(1), 133-156.</p>
					<p>Kutas, M., & Hillyard, S. A. (1984). Brain potentials during reading reflect word expectancy and semantic association. <em>Nature</em>, 307(5947), 161-163.</p>
					<p>Lew-Williams, C., & Fernald, A. (2007). Young children learning Spanish make rapid use of grammatical gender in spoken word recognition. <em>Psychological science</em>, 18(3), 193-198.</p>
					<p>Lew-Williams, C., & Fernald, A. (2010). Real-time processing of gender-marked articles by native and non-native Spanish speakers. <em>Journal of memory and language</em>, 63(4), 447-464.</p>
					<p>Nieuwland, M. S., Politzer-Ahles, S., Heyselaar, E., Segaert, K., Darley, E., Kazanina, N., ... & Huettig, F. (2018). Large-scale replication study reveals a limit on probabilistic prediction in language comprehension. <em>ELife</em>, 7, e33468.</p>
					<p>Shun, L., Chen, X., & Wang, S. (2023). <em>The involvement of phonological information during spoken language prediction: evidence based on Chinese tone sandhi</em>. OSF Preprints.</p>
				</section>
			</div>
			<div class="logo"></div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script src="plugin/zoom/zoom.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				slideNumber: 'c/t',
				controls: false,
				hash: true,
				loop: true,
				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX, RevealZoom ]
			});
		</script>
	</body>
</html>
